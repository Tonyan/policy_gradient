<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="./img/favicon.ico">

	<title>Policy Gradient</title>

        <link href="./css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="./css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="./css/highlight.css">
        <link href="./css/base.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href=".">Policy Gradient</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li class="active">
                    <a href=".">Home</a>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                <li class="disabled">
                    <a rel="next" >
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li class="disabled">
                    <a rel="prev" >
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#introduction">Introduction</a></li>
        
    
        <li class="main "><a href="#installation">Installation</a></li>
        
    
        <li class="main "><a href="#interface">Interface</a></li>
        
    
        <li class="main "><a href="#main-functions">Main Functions</a></li>
        
            <li><a href="#9595init9595self-net_dims-output_functionnone">__init__(self, net_dims, output_function=None)</a></li>
        
            <li><a href="#train_agentself-dynamics_func-reward_func-initial_state-num_iters-batch_size-traj_len-step_size01-momentum05-normalizetrue">train_agent(self, dynamics_func, reward_func, initial_state, num_iters, batch_size, traj_len, step_size=0.1, momentum=0.5, normalize=True)</a></li>
        
            <li><a href="#gradient_updateself-traj_states-traj_actions-rewards-step_size-momentum-normalize">gradient_update(self, traj_states, traj_actions, rewards, step_size, momentum, normalize)</a></li>
        
            <li><a href="#get_actionself-state">get_action(self, state)</a></li>
        
    
        <li class="main "><a href="#utility-functions">Utility Functions</a></li>
        
            <li><a href="#init_neural_netself-net_dims-output_functionnone">init_neural_net(self, net_dims, output_function=None)</a></li>
        
            <li><a href="#setup_netself-net_dims">setup_net(self, net_dims)</a></li>
        
            <li><a href="#estimate_qself-states-actions-rewards-net_dimsnone">estimate_q(self, states, actions, rewards, net_dims=None)</a></li>
        
            <li><a href="#meanstd_sampleself-mean-std10">meanstd_sample(self, mean, std=1.0)</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="introduction">Introduction</h1>
<p>Python implementation of policy gradient reinforcement learning methods, built on TensorFlow. </p>
<p>Flexible reinforcement learning framework that does not require dynamics of the environment. </p>
<p>Given a sequence of state-action pairs and rewards, adjusts policy based on gradient steps with respect to its parameters. </p>
<p>Interface designed to minimize the amount of developer interaction needed. </p>
<h1 id="installation">Installation</h1>
<blockquote>
<p>pip install -i https://testpypi.python.org/pypi policy_gradient</p>
</blockquote>
<p>Quick installation using the pip package manager.
<aside class="notice">
Dependencies: NumPy, TensorFlow, Nose
</aside></p>
<h1 id="interface">Interface</h1>
<p>Quick, clean interface for training a neural network using policy gradient methods.</p>
<p>Designed to minimize the amount of interaction with the interface. </p>
<p>Just initialize the learner, pass in the dynamics/rewards functions and the initial state,
and you're good to go!</p>
<pre><code class="python"># Initialize learner
learner = PolicyGradient(net_dims, 'tanh')
# Train policy
learner.train_agent(dynamics_func, reward_func, initial_state)
# Retrieve predicted best actions based on learned policy
learner.get_action(new_state)
</code></pre>

<h1 id="main-functions">Main Functions</h1>
<p>Calculates policy gradient for given input state/actions.</p>
<p>Users should primarily be calling main PolicyGradient class methods.</p>
<h2 id="9595init9595self-net_dims-output_functionnone">__init__(self, net_dims, output_function=None)</h2>
<p>Initializes PolicyGradient class.</p>
<h3 id="inputs">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>net_dims</td>
<td>array-like</td>
<td>1D list corresponding to dimensions of each layer in the net.</td>
</tr>
<tr>
<td>output_function</td>
<td>string</td>
<td>Non-linearity function applied to output of neural network. Options are: 'tanh', 'sigmoid', 'relu'.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Dimensions of net
net_dims = np.array([10,25,15,5,1])
pol_grad = PolicyGradient(net_dims, output_function='tanh')
</code></pre>

<h2 id="train_agentself-dynamics_func-reward_func-initial_state-num_iters-batch_size-traj_len-step_size01-momentum05-normalizetrue">train_agent(self, dynamics_func, reward_func, initial_state, num_iters, batch_size, traj_len, step_size=0.1, momentum=0.5, normalize=True)</h2>
<p>Trains agent using input dynamics and rewards functions.</p>
<h3 id="inputs_1">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>dynamics_func</td>
<td>function</td>
<td>User-provided function that takes in a state and action, and returns the next state.</td>
</tr>
<tr>
<td>reward_func</td>
<td>function</td>
<td>User-provided function that takes in a state and action, and returns the associated reward.</td>
</tr>
<tr>
<td>initial_state</td>
<td>array-like</td>
<td>Initial state that each trajectory starts at. Must be 1-dimensional NumPy array.</td>
</tr>
<tr>
<td>num_iters</td>
<td>int</td>
<td>Number of iterations to run gradient updates.</td>
</tr>
<tr>
<td>batch_size</td>
<td>int</td>
<td>Number of trajectories to run in a single iteration.</td>
</tr>
<tr>
<td>traj_len</td>
<td>int</td>
<td>Number of state-action pairs in a trajectory.</td>
</tr>
</tbody>
</table>
<h3 id="outputs">Outputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>mean_rewards</td>
<td>array-like</td>
<td>Mean ending rewards of all iterations.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Setup initial state
initial_state = np.zeros((5, 5))
mean_rewards = pol_grad.train_agent(dynamics_func, reward_func, initial_state=initial_state, \
    num_iters=100, batch_size=50, traj_len=10)
</code></pre>

<h2 id="gradient_updateself-traj_states-traj_actions-rewards-step_size-momentum-normalize">gradient_update(self, traj_states, traj_actions, rewards, step_size, momentum, normalize)</h2>
<p>Estimates and applies gradient update according to a policy.</p>
<p>States, actions, rewards must be lists of lists; first dimension indexes
the ith trajectory, second dimension indexes the jth state-action-reward of that
trajectory.</p>
<h3 id="inputs_2">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>traj_states</td>
<td>array-like</td>
<td>List of list of states.</td>
</tr>
<tr>
<td>traj_actions</td>
<td>array-like</td>
<td>List of list of actions.</td>
</tr>
<tr>
<td>rewards</td>
<td>array-like</td>
<td>List of list of rewards.</td>
</tr>
<tr>
<td>step_size</td>
<td>float</td>
<td>Step size.</td>
</tr>
<tr>
<td>momentum</td>
<td>float</td>
<td>Momentum value.</td>
</tr>
<tr>
<td>normalize</td>
<td>boolean</td>
<td>Determines whether to normalize gradient update. Recommended if running into NaN/infinite value errors.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Obtain traj_states, traj_actions, rewards through environment
pol_grad.gradient_update(traj_states, traj_actions, rewards)
</code></pre>

<h2 id="get_actionself-state">get_action(self, state)</h2>
<p>Returns action based on input state.</p>
<h3 id="inputs_3">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>state</td>
<td>array-like</td>
<td>Input state.</td>
</tr>
</tbody>
</table>
<h3 id="outputs_1">Outputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>action</td>
<td>array-like</td>
<td>Predicted action.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Arbitrary input state
new_state = np.array([[10,20],[20,10]])
next_action = pol_grad.get_action(new_state)
</code></pre>

<h1 id="utility-functions">Utility Functions</h1>
<h2 id="init_neural_netself-net_dims-output_functionnone">init_neural_net(self, net_dims, output_function=None)</h2>
<p>Sets up neural network for policy gradient.</p>
<h3 id="inputs_4">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>net_dims</td>
<td>array-like</td>
<td>List of dimensions for layers of net.</td>
</tr>
<tr>
<td>output_function</td>
<td>string</td>
<td>Non-linearity function applied to output of neural network. Options are: 'tanh', 'sigmoid', 'relu'.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Dimensions of net
net_dims = np.array([10,25,15,5,1])
pol_grad.init_neural_net(net_dims, output_function=None)
</code></pre>

<h2 id="setup_netself-net_dims">setup_net(self, net_dims)</h2>
<p>Initializes TensorFlow neural net with input net dimensions.</p>
<h3 id="inputs_5">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>net_dims</td>
<td>array-like</td>
<td>List of dimensions for layers of net.</td>
</tr>
</tbody>
</table>
<h3 id="outputs_2">Outputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>layers</td>
<td>array-like</td>
<td>List of TensorFlow nodes corresponding to layers of net.</td>
</tr>
<tr>
<td>weights</td>
<td>array-like</td>
<td>List of net weights.</td>
</tr>
<tr>
<td>biases</td>
<td>array-like</td>
<td>List of net biases.</td>
</tr>
<tr>
<td>input_state</td>
<td>TensorFlow node</td>
<td>Placeholder for input state.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Dimensions of net
net_dims = np.array([10,25,15,5,1])
layers, weights, biases, input_state = pol_grad.setup_net(net_dims)
</code></pre>

<h2 id="estimate_qself-states-actions-rewards-net_dimsnone">estimate_q(self, states, actions, rewards, net_dims=None)</h2>
<p>Estimates the q-values for a trajectory based on the intermediate rewards.</p>
<h3 id="inputs_6">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>states</td>
<td>array-like</td>
<td>List of list of input states.</td>
</tr>
<tr>
<td>actions</td>
<td>array-like</td>
<td>List of list of input actions.</td>
</tr>
<tr>
<td>rewards</td>
<td>array-like</td>
<td>List of list of input rewards.</td>
</tr>
<tr>
<td>net_dims</td>
<td>array-like</td>
<td>List of dimensions for layers of net. Defaults to three layer net, with dimensions  x, 2x, 1.5x, 1 respectively where x is the length of the trajectory.</td>
</tr>
</tbody>
</table>
<h3 id="outputs_3">Outputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>q</td>
<td>array-like</td>
<td>Estimated q-values.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Obtain traj_states, traj_actions, rewards through environment
q_value_lists = pol_grad.estimate_q(states, actions, rewards, net_dims=None)
</code></pre>

<h2 id="meanstd_sampleself-mean-std10">meanstd_sample(self, mean, std=1.0)</h2>
<p>Samples an action based on the input probability distribution.</p>
<h3 id="inputs_7">Inputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>mean</td>
<td>array-like</td>
<td>Input mean of action distribution.</td>
</tr>
<tr>
<td>std</td>
<td>float</td>
<td>Standard deviation of action distribution.</td>
</tr>
</tbody>
</table>
<h3 id="outputs_4">Outputs</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>sample_action</td>
<td>array-like</td>
<td>Action sampled from given distribution.</td>
</tr>
</tbody>
</table>
<pre><code class="python"># Obtain predicted action through net
new_action_mean = pol_grad.sess.run(pol_grad.output_mean, feed_dict={self.input_state: state})
action_sampled = pol_grad.meanstd_sample(mean, std=1.0)
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <center>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</center>
        </footer>

        <script src="./js/jquery-1.10.2.min.js"></script>
        <script src="./js/bootstrap-3.0.3.min.js"></script>
        <script src="./js/highlight.pack.js"></script>
        <script>var base_url = '.';</script>
        <script data-main="./mkdocs/js/search.js" src="./mkdocs/js/require.js"></script>
        <script src="./js/base.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            From here you can search these documents. Enter
                            your search terms below.
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>